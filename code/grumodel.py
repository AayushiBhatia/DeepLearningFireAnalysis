# -*- coding: utf-8 -*-
"""ThailandFC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gubnvxHG3h4kw2IiDWUkk6nptI9MZJJI
"""

!pip install scikeras[tensorflow]

pip install tensorflow keras

!pip install --upgrade scikit-learn

import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, GRU, Dense, Dropout
from scikeras.wrappers import KerasRegressor
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from tensorflow.keras.optimizers import Adam, SGD, Adadelta, Adagrad, Adamax, Nadam, RMSprop, Ftrl

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
import random
from keras.layers import Input, GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam, SGD, Adadelta, Adagrad, Adamax, Nadam, RMSprop, Ftrl
from tensorflow.keras.models import Sequential
from keras.callbacks import EarlyStopping
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.layers import GRU, Dense
from sklearn.metrics import mean_squared_error
from tensorflow.keras.layers import GRU, Dense, Dropout
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

from google.colab import drive
drive.mount('/content/drive')

dt = pd.read_csv('/content/drive/MyDrive/NSSTC/DeepLearning/SEAsiaGRU/Thailand/Thailand_FC.csv', sep = ',')
dt

dates = data[:'2019-12']

dates.drop('FCSUM', axis =1)

data = dt.copy()

np.random.seed(24)
tf.random.set_seed(24)
random.seed(24)

data['DATE'] = pd.to_datetime(data['DATE'])
data.set_index('DATE', inplace=True)

train_data = data[:'2019-12']
test_data = data['2020-01':]

scaler = MinMaxScaler(feature_range=(0, 1))
train_scaled = scaler.fit_transform(train_data)
test_scaled = scaler.transform(test_data)

def create_sequences(data, time_steps=1):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:(i + time_steps), 0])
        y.append(data[i + time_steps, 0])
    return np.array(X), np.array(y)

time_steps = 4  # Example time step
X_train, y_train = create_sequences(train_scaled, time_steps)
X_test, y_test = create_sequences(test_scaled, time_steps)

# Reshape input data to 3D for GRU [samples, time steps, features]
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

model = Sequential()
model.add(GRU(50, return_sequences=True, input_shape=(time_steps, 1)))
model.add(GRU(50))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), verbose=2)

# Make predictions
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# Inverse scaling
train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)
y_train = scaler.inverse_transform([y_train])
y_test = scaler.inverse_transform([y_test])

train_plot = np.empty_like(data['FCSUM'], dtype=np.float32)
train_plot[:] = np.nan
train_plot[time_steps:len(train_predict)+time_steps] = train_predict.flatten()

# Adjust the slicing indices to match the shape of test_predict
test_plot = np.empty_like(data['FCSUM'], dtype=np.float32)
test_plot[:] = np.nan
test_plot[len(train_predict)+(time_steps*2):len(train_predict)+(time_steps*2)+len(test_predict)] = test_predict.flatten()

# Plot the true values, train predictions, and test predictions
plt.figure(figsize=(14, 8))
plt.plot(data.index, data['FCSUM'], label='Actual Data',marker='o', color = 'blue')
plt.plot(data.index, train_plot, label='Fit Curve', linestyle = '--',marker='x',color = 'orange')
plt.plot(data.index, test_plot, label='Test Predictions', marker='x', color= 'green')
plt.title('GRU Model Predictions')
plt.xlabel('Date')
plt.ylabel('FireCount')
plt.legend()
plt.show()

plt.figure(figsize=(14, 8))
plt.plot(data.index, data['FCSUM'], label='Actual Data', color = 'blue')
plt.plot(data.index, train_plot, label='Fit Curve', linestyle = '--',color = 'orange')
plt.plot(data.index, test_plot, label='Test Predictions', color= 'green')
plt.title('GRU Model Predictions')
plt.xlabel('Date')
plt.ylabel('FireCount')
plt.legend()
plt.show()

plt.figure(figsize=(8, 4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

test_predict = test_predict.flatten()
y_test = y_test.flatten()

# Compute RMSE
test_rmse = np.sqrt(mean_squared_error(y_test, test_predict))
print(f'Test RMSE: {test_rmse}')

future_df.to_csv('/content/drive/MyDrive/NSSTC/DeepLearning/SEAsiaGRU/Cambodia/cambodia_futurepred.csv', index=True)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np


mse_train = mean_squared_error(y_train.flatten(), train_predict.flatten())
mse_test = mean_squared_error(y_test.flatten(), test_predict.flatten())

mae_train = mean_absolute_error(y_train.flatten(), train_predict.flatten())
mae_test = mean_absolute_error(y_test.flatten(), test_predict.flatten())

r2_train = r2_score(y_train.flatten(), train_predict.flatten())
r2_test = r2_score(y_test.flatten(), test_predict.flatten())

test_predict = test_predict.flatten()
y_test = y_test.flatten()

test_rmse = np.sqrt(mean_squared_error(y_test, test_predict))


# Print the results
print(f'MSE (Test): {mse_test}')
print(f'MAE (Test): {mae_test}')
print(f'RÂ² (Test): {r2_test}')
print(f'RMSE (Test): {test_rmse}')

dt

test_data

GRUResult = pd.DataFrame ({
                                   'Actual': y_test.flatten(),
                                   'Predicted': test_predict.flatten()})

GRUResult

LSTMResults.to_csv('/content/drive/MyDrive/NSSTC/DeepLearning/SEAsiaGRU/Thailand/LSTMResults1.csv', index=True)